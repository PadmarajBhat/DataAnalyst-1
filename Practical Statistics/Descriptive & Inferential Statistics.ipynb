{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Broadly there are 2 types data\n",
    "* Quantitative Data: number of dogs - has numerical data allows to do the mathematical operation on them\n",
    "* Categorical data: type of dogs - label the group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data can be devided into : \n",
    "* categorical ordinal data: order in the values like rating out of 5. \n",
    "* categorical nominal data: data are grouped but there is no order like breeds of dogs. Rating for dog breed is indivisual choice. Price for the dog breed is set based on demand and supply ration but that does not make the categorical names of the breed any great or less or order in the \"names\".\n",
    "\n",
    "\n",
    "In our kaggle hourse price prediction challenge, we try to find ordinality in the columns like vnr_type. Just because we could represent the number in ordinality factor and then perform xgboost on them as if it is continuous data. \n",
    "\n",
    "IS IT CORRECT APPROACH ? IS IT TOO EARLY to ask ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantitative data can be devided into :\n",
    "* continuous data: can vary from -ve to +ve and can be infinitely (not literally) small.\n",
    "* descrete data: whole number but there are many values. E.g.: number of dogs cannot be 4.5. However, count can be be even in millions.\n",
    "\n",
    "Now between ordinal and descrete: \n",
    "* ordinal has limited set of value. e.g.: Dog breed name\n",
    "* descrete are indivisual values with may be logical boundaries (like dogs cannot have 5 legs though a cut leg can yield 3 legs...here cut leg cannot be half leg as it is not a leg.) Ask 2 questions: can I count it? can it be broken into smaller pieces. if the answer is yes and then no then vola!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Main Aspects of Quantitative Data\n",
    "* Center\n",
    "* Spread\n",
    "* Shape\n",
    "* Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Center : \n",
    "* Mean: average,\n",
    "* Median: middle value, \n",
    "* Mode: most occurring value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notations:\n",
    "* X: capital indicates the set of variables\n",
    "* x: small letter indicate the value in the set\n",
    "* Summation indication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\sum_{i=1}^n x_i\n",
    "\\end{equation*}\n",
    "\n",
    "   * Here i, varies from 1 to n and n can be any whole number\n",
    "   \n",
    "\\begin{equation*}\n",
    "\\bar x\n",
    "\\end{equation*}\n",
    "   * Here x bar indicates mean\n",
    "   \n",
    "* Hence, mean formula :\n",
    "\\begin{equation*}\n",
    "\\bar x = \\frac{1}{n} \\left( \\sum_ {i=1}^n x_i \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spread: Range, Interquartile Range, Standard Deviation and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Histogram** is bin based and comparable with other variables for the height and width of it\n",
    "* **Box-Plot**: displays the Q3 and Q1 in a box like structure. It is like histogram turned 90 degrees.\n",
    "* **5 Number summary** : min, Q1, Q2 ( median), Q3, Max ; Q1 and Q3 are the median for the first and second half of the variable.\n",
    "#####    \n",
    "* **Range**: max - min\n",
    "* **IQR**: Q3 -Q2\n",
    "* **Standard Deviation**: On an average how dataset varies from the mean\n",
    "* **Variance**: Average squared deviation from the mean. Square of Standard deviation. However, first variance is calcuated and the SD is calculated by taking square root of it.\n",
    "\n",
    "    * Variance for Sample dataset:\n",
    "\\begin{equation*}\n",
    "variance = \\frac{1}{n-1} \\left( \\sum_ {i=1}^n \\left( x_i - \\bar x \\right)^2 \\right)\n",
    "\\end{equation*}\n",
    "#####   \n",
    "    * Variance for population:\n",
    "\\begin{equation*}\n",
    "variance = \\frac{1}{n} \\left( \\sum_ {i=1}^n \\left( x_i - \\bar x \\right)^2 \\right)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Shape: Right Skewed, Left Skewed and symmetric\n",
    "* Bi-modal means there is 2 histogram like structure in one plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Outlier: \n",
    "* Noting that it exists will help us know that it impacts statistics\n",
    "* if human error can be fixed like inches to cm entry, character like km in kilometer field\n",
    "* As it impacts the statistics, need to be extra careful while creating reports.\n",
    "\n",
    "There are tests like in the below blog (normaltest -scipy) to determine if the distribution is normal. \n",
    "https://machinelearningmastery.com/a-gentle-introduction-to-normality-tests-in-python/\n",
    "\n",
    "If the distribution is normal then the mean and standard deviation can represent the entire dataset. Hence, safeguarding from the outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to inferential statistics:\n",
    "* **Inferential statistics uses descriptive statics of sample to draw conclusion on the population**\n",
    "* **statistics**: numerical summary of sample\n",
    "* **parameter**: statistics value of the population\n",
    "* **sample**: subset of population\n",
    "* **population**: entire dataset/ group of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability\n",
    "* **Simpson paradox**: indivisual aggregation might look large but cumulated aggregation looks small. Problem talks about bias in gender for college admission. When seen from the department level, it appears as if it is female's getting higher admission but when aggregation is taken over all the department then it appears as if males have higher chance of admission when we see that total of both male and female applications are same.\n",
    "\n",
    "* **Probability** is to predict the future and **Statistics** is to learn from the history\n",
    "* **Probability of two instances occurring together** is the product of 2 indivisual probabilities and can be verified through truth table. **Probability of the composite events** is the product of those indivisual probabilites.\n",
    "* **Probability of the opposite event** is 1-P(first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Binomial Distribution: a frequency distribution of the possible number of successful outcomes in a given number of trials in each of which there is the same probability of success.\n",
    "\\begin{equation*}\n",
    "P(X=x) = \\frac{n!}{x!(n-x)!} \\left(  {p}^x \\left( 1 - p \\right)^{n-x} \\right)\n",
    "\\end{equation*}\n",
    "* X = an event\n",
    "* x = number of successful outcome\n",
    "* n = number of events\n",
    "* p = P(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Conditional Probability**: \n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)} \n",
    "\\end{equation*}\n",
    "\n",
    "    * P(A|B) = Probability of the event 'A' occurring when we know that 'B' event has occurred.\n",
    "    * P(A AND B) = Probability of the both event occurring together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bayes Rule**:\n",
    "\\begin{equation*}\n",
    "P(A|B) = \\frac{P(B|A) P(A)}{P(B)} \n",
    "\\end{equation*}\n",
    "\n",
    "* On a four-lane highway, cars are either going fast or not fast. Faster cars should go in the leftmost lanes.\n",
    "\n",
    "    * At any given time, 20% of cars are in the left-most lane.\n",
    "\n",
    "    * Overall, 40% of cars on the highway are classified as going fast.\n",
    "\n",
    "    * Out of all the cars in the leftmost lane, 90% are going fast.\n",
    "\n",
    "P(L) = 0.2\n",
    "\n",
    "P(F) = 0.4\n",
    "\n",
    "P(F|L) = 0.9\n",
    "\n",
    "* Given the above information, if a car is going fast, what is the probability that it will be in the leftmost lane?\n",
    "\n",
    "    * P(L|F) = 0.9 * 0.2 / 0.4 = 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1% of all people have cancer.\n",
    "    * P(C) = .01\n",
    "* 90% of people who have cancer test positive when given a cancer-detecting blood test, meaning the test detects cancer 90% of the time.\n",
    "    * P(Pos|C) =.90\n",
    "* 5% of people will have false positives, meaning that 5% of the time, this test will produce a positive result when people do not have cancer\n",
    "    * P(pos| ~c) = .05\n",
    "\n",
    "\n",
    "* Given the above data, what is the probability that a person has cancer if they have a positive cancer-test result? (Note: answers are rounded to the nearest 4th decimal place).\n",
    "\n",
    "    * P(C|pos) = P(pos|c) P(c) / P(pos)\n",
    "    * p(pos) = P(pos|c) * p(c) + p(pos|~c) p(~c)\n",
    "\n",
    "\t\t\n",
    "    * P(C|pos) = (.9 * .01)/ ((0.9 * 0.01) + (0.05 * .99)) = 0.1538"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Distribution\n",
    "\n",
    "\\begin{equation*}\n",
    "N({X}_i, \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi{\\sigma}^2}} {e}^{\\frac{-1}{2} \\frac{\\left(X - \\mu\\right)^2}{{\\sigma}^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* **Quadratic Penalty term of the deviation from expectaion of the mean of the expression and exponential actually squeezes  it back to curves.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling:\n",
    "\n",
    "* **Statistics**: Numeric summary of the sample\n",
    "* **Sampling distribution**: distribution of statistics\n",
    "* **parameter** : Numeric summary of the population\n",
    "* **bootstrapping** : sampling with replacement\n",
    "\n",
    "\n",
    "\n",
    "* **Law of Large Number theorem**: Larger the sample size, closer the mean of the sample to that of population\n",
    "* **Central Limit Theorem**: Larger the sample size, sample mean follows normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample is a portion of Population. In theory, there exist a location where the population resides. This might not be true for all the practical cases. I cannot think of a database where entire print paper consumption per IT company being present. But I can have one dataset where a print paper consumption for couple of IT companies (let us say 5) willing to share this detail.\n",
    "\n",
    "\n",
    "\n",
    "Any Statistics: Now for this print paper there can be lot of statistics (what are they?). Mean paper consumption is one of them.\n",
    "\n",
    "In pursuit of identifying the mean print consumption of all the companies. We perform sampling on  available 5 companies paper consumption. if the company names are (A, B, C, D, E) then samples can be (A,B), (A,B,C), (A,C,D,E)...so on. And for each of those samples we compute mean paper consumption.\n",
    "\n",
    "But we clearly know that smaller set would not give that info and large number theorem states we should have bigger sample size better is outcome.\n",
    "\n",
    "For this, we use bootstrapping where we take all the available sample items with replacement. For e.g. 5 companies are drawn with replacement. e.g. (A,A,B,B,D) and so on. [Would not it be equivalent to (A,B,D) ???? and Does bootstrap sample size = max all the time ? or it can be less than available sample size ??]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([1,2,3]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,1,2,3]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 1. Anwer to the silly question, whether (A,A,B,B,C) == (A,B,C)? is no. The above 2 cells gives answer. If you put(A,A,B,B,C) in a set it would lead to (A,B,C) but the mean computation is not like that :P\n",
    "* 2. Answer to the second question, whether bootstrap size has to be all the max available sample. Need not have to be. However, having max size helps as per the law large number theorem.\n",
    "\n",
    "Source: machinelearningmastery.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jason Brownlee also tells that the number of bootstrapped samples also must be high as possible so as to accurately get the statistics like mean, variance and standard error. This answers our initial question of what are the statistics we are targetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the normal distribution from many bootstrapped sampling (called as sampling distribution which follows normal distribution if the sample size is large - central limit theorem),\n",
    "\n",
    "we can get the confidence interval by chopping the confidence interval equally at the both end of the normal distribution. The depth of chop depends on the required confidence interval.\n",
    "\n",
    "for e.g.: if we want 95% confidence interval then we have to chop at both end of the normal distribution of the sample distribution by 2.5%.\n",
    "\n",
    "*What does the confidence interval give us ?*\n",
    "* It give the region or the interval or range of values between which lies the parameter (mean, standard deviation and standard error) of the **population**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical and Practical Inferences**:\n",
    "\n",
    "* Statistical: which website gets more hit between the 2.\n",
    "* Practical: though the second website has more hits, it is time consuming and also costlier. Therefore, go ahead with first website only when both generate enough ROI (return on investment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence Interval**: bigger the sampling size narrower the confidence interval. Narrower confidence interval can also be viewed as narrow the marginal error. \n",
    "\n",
    "**How to interpret the confidence Interval**: It talks about the interval/range where the statics of the population (or parameter of the population) like mean, standard deviation and difference in mean of the sample and population can be found.\n",
    "\n",
    "It does not say anything about what would one indivisual score/outcome in the population. It talks about aggregated summary.\n",
    "\n",
    "If we casually say that a percular party wins an election then we usually talk about mode or average. Or it can be your biased due to thier propaganda through social media. But an old person talking about his experience on a task then it is usually mean or mode (assuming that he is not bluffing). If that is so, his different experience in that task can be a sample and definitely not population. In this context we can say that there is exist a confidence interval rather than saying whatever S/he says as the actual outcome. That confidence interval in practial aspects depends on his past claim on other tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis Testing:\n",
    "\n",
    "* We need to represent a business question in the form of 2 hypotheses.\n",
    "    * H0 = Null Hypothesis => mathematically represented as == or <= or >= between 2 conditions in the question\n",
    "    * H1 = alternate hypothesis => mathematically represented as != or < or > between 2 conditions in the question\n",
    "    * Hypothesis are always on parameter (for population) and not for the statics of the sample because we have them with us as truthful data.\n",
    "    \n",
    "        * e.g: Business question: is cow milk better taste quality than packet milk ?\n",
    "        * H0 : Both cow milk and packet milk are of same taste quality\n",
    "        * H1 : cow milk has better taste quality than the packet milk.\n",
    "\n",
    "\n",
    "* There are 2 types of errors in hypothesis testing:\n",
    "    * Type 1: Highest severity : notation alpha: False Positive\n",
    "    * Type 2: comparitively low severity: notation beta: True Negative\n",
    "\n",
    "\n",
    "\n",
    "* In order to be extra cautious to get away from Type 1 error we have to carefully select the hypotheses. In our previous example, H0 and H1 has to be reversed because later is the new one and cow milk was there from day one.\n",
    "\n",
    "\n",
    "* To prove the hypothesis we have 2 approaches:\n",
    "    * confidence interval to check the hypothesis align to it. \n",
    "        * compute the sampling distibution through bootstrapping and compute the parameter and prove the hypothesis either true or flase.\n",
    "    * Simulating what is possible under the null hypothesis and then seeing if the data is actually is consistent with that.\n",
    "        * compute the mean and standard deviation of the sample and then at the corner of distribution identify a sample which has high standard deviation and prove that null hypothesis border is crossed.\n",
    "        \n",
    "        \n",
    "        \n",
    "* p-value: If the H0 (null hypothesis) is true, what is the probability of observed statistics or one more extreme in the favor of alternate hypothesis. if the alternate hypothesis fails then the p value is large and if the null hypothesis fails  then p-value is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
